
只对lagrCap部分进行了优化，具体对函数的分析可见"原项目+注释"版本，与"gpu优化"版本结合，便于理解


1.对于所有前后不冲突循环进行了并行优化，多以空间换时间：
    其中对于简单函数，如数组赋值、数组加减，调用核函数的花销与并行节省的花销需要比较权衡。
        对于循环间接加法，如a[b[i]]++，将b[i]先转换为矩阵形式：c[i][j]=1当j=b[i]，再规约为a[j]
    复杂函数中，根据理论分析，并行优化的效率主要取决于背包过程，此处将其中的Kcap优化为log(Kcap)
        对于Lagrangian中的背包进行了在dp过程中kcap个操作并行的优化，以及sub_problem中m个工厂并行的优化，为方便归约，对数组维的顺序进行了调整；
        对于fixSol中的排序操作，调用了thrust中sort_by_key函数
            此处不能使用原项目中的compCap匿名函数，因为它在gpu调用需要声明__device__，而该声明不能在函数内进行，而它引用了函数中创建的capres数组
        对于LocalSearch中的搜索，进行了m个工厂对于n个工作同时搜索的优化，并取更新后最小的解来继续更新
2.对于找最值及位置，创建了模板类node；对于常用函数进行了模板封装（主要为了方便reduction过程），在node.cuh、node.cu中可见其声明与定义。
    其中模板类及模板函数，应用时需要进行实例化，该操作不能在头文件中进行，故在node.cu中实例化，在node.cu中extern来防止编译中的多重定义
    对于node中重载=运算符，不能在模板类中进行（会出不明错误），故设置了copy函数，来进行node的赋值
    对于模板函数，原申请共享内存方式不可用，需要通过SharedMemory进行申请
3.对于核函数进行了封装，在gpu.cuh、gpu.cu中可见其声明与定义
    对于reduction过程，由于样例中含有1600个工作，超过了机器每个block最大的thread数；又为了可扩展，故设置了do_reduction来进行操作
4.对于变量的转换，在gpu中不能访问内存中的变量及数组，反之亦然（可以传送指针，但是读取时会出错）
    由于对于数组的操作均封装为核函数、在gpu上进行，故在读取jaon文件时所有的数组都设为device的数组
    不做修改的变量可通过参数、返回值来进行host与device之间的交流
    对于zlb等原工程中需要在gpu中引用修改的host变量，设置了get_temp与delete_temp函数申请gpu空间来操作
        也可以通过__device__变量，通过运行时库中的cudaMemcpyFromSymbol、cudaGetSymbolAddress来更改变量
6.函数修饰符及其使用条件
    __device__：在gpu进行、由gpu调用（__device__、__global__）
    __global__：在gpu进行、由cpu调用（__host__）
    __host__：在cpu进行、由cpu调用（__host__）
    未进行修饰的函数均默认为__host__

7.其他：
    对于引用的头文件，为方便运行，将其放在了inc文件夹中
    可更改makefile中的SMS来编译本机可运行的执行文件
    使用make编译，argv参数为输入文件的相对路径
    对于输入文件参数选择-dc来进行分散编译，否则会链接失败

    
